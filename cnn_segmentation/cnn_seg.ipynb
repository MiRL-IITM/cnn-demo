{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "\n",
    "IMAGE_PATH = \"data/dataset/semantic_drone_dataset/original_images/\"\n",
    "MASK_PATH = \"data/dataset/semantic_drone_dataset/label_images_semantic/\"\n",
    "\n",
    "\n",
    "def create_df():\n",
    "    name = []\n",
    "    for dirname, _, filenames in os.walk(IMAGE_PATH):\n",
    "        for filename in filenames:\n",
    "            name.append(filename.split(\".\")[0])\n",
    "    return pd.DataFrame({\"id\": name}, index=np.arange(0, len(name)))\n",
    "\n",
    "\n",
    "def get_data_splits():\n",
    "    df = create_df()\n",
    "    X_trainval, X_test = train_test_split(\n",
    "        df[\"id\"].values, test_size=0.1, random_state=19\n",
    "    )\n",
    "    X_train, X_val = train_test_split(X_trainval, test_size=0.15, random_state=19)\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "\n",
    "class DroneDataset(Dataset):\n",
    "    def __init__(self, img_path, mask_path, X, mean, std, transform=None, patch=False):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "        self.patches = patch\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_path + self.X[idx] + \".jpg\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_path + self.X[idx] + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img, mask=mask)\n",
    "            img = Image.fromarray(aug[\"image\"])\n",
    "            mask = aug[\"mask\"]\n",
    "\n",
    "        if self.transform is None:\n",
    "            img = Image.fromarray(img)\n",
    "\n",
    "        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n",
    "        img = t(img)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        if self.patches:\n",
    "            img, mask = self.tiles(img, mask)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "    def tiles(self, img, mask):\n",
    "        img_patches = img.unfold(1, 512, 512).unfold(2, 768, 768)\n",
    "        img_patches = img_patches.contiguous().view(3, -1, 512, 768)\n",
    "        img_patches = img_patches.permute(1, 0, 2, 3)\n",
    "\n",
    "        mask_patches = mask.unfold(0, 512, 512).unfold(1, 768, 768)\n",
    "        mask_patches = mask_patches.contiguous().view(-1, 512, 768)\n",
    "\n",
    "        return img_patches, mask_patches\n",
    "\n",
    "\n",
    "class DroneTestDataset(Dataset):\n",
    "    def __init__(self, img_path, mask_path, X, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_path + self.X[idx] + \".jpg\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_path + self.X[idx] + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img, mask=mask)\n",
    "            img = Image.fromarray(aug[\"image\"])\n",
    "            mask = aug[\"mask\"]\n",
    "\n",
    "        if self.transform is None:\n",
    "            img = Image.fromarray(img)\n",
    "\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "def get_data_loaders(batch_size=16):\n",
    "    X_train, X_val, X_test = get_data_splits()\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    t_train = A.Compose(\n",
    "        [\n",
    "            A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST),\n",
    "            A.HorizontalFlip(),\n",
    "            A.VerticalFlip(),\n",
    "            A.GridDistortion(p=0.2),\n",
    "            A.RandomBrightnessContrast((0, 0.5), (0, 0.5)),\n",
    "            A.GaussNoise(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    t_val = A.Compose(\n",
    "        [\n",
    "            A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST),\n",
    "            A.HorizontalFlip(),\n",
    "            A.GridDistortion(p=0.2),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_set = DroneDataset(\n",
    "        IMAGE_PATH, MASK_PATH, X_train, mean, std, t_train, patch=False\n",
    "    )\n",
    "    val_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_val, mean, std, t_val, patch=False)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    t_test = A.Resize(768, 1152, interpolation=cv2.INTER_NEAREST)\n",
    "    test_set = DroneTestDataset(IMAGE_PATH, MASK_PATH, X_test, transform=t_test)\n",
    "\n",
    "    return train_loader, val_loader, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes):  # loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0:  # no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = (\n",
    "                    torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                )\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union + smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from metrics import mIoU, pixel_accuracy\n",
    "import os\n",
    "from torchvision import transforms  # Change this line\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    return smp.Unet(\n",
    "        \"mobilenet_v2\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        classes=23,\n",
    "        activation=None,\n",
    "        encoder_depth=5,\n",
    "        decoder_channels=[256, 128, 64, 32, 16],\n",
    "    )\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "\n",
    "def fit(\n",
    "    epochs,\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    patch=False,\n",
    "    checkpoint_dir=\"checkpoints\",\n",
    "):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_iou = []\n",
    "    val_acc = []\n",
    "    train_iou = []\n",
    "    train_acc = []\n",
    "    lrs = []\n",
    "    min_loss = np.inf\n",
    "    decrease = 1\n",
    "    not_improve = 0\n",
    "\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    model.to(device)\n",
    "    fit_time = time.time()\n",
    "    for e in range(epochs):\n",
    "        since = time.time()\n",
    "        running_loss = 0\n",
    "        iou_score = 0\n",
    "        accuracy = 0\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for i, data in enumerate(tqdm(train_loader)):\n",
    "            # training phase\n",
    "            image_tiles, mask_tiles = data\n",
    "            if patch:\n",
    "                bs, n_tiles, c, h, w = image_tiles.size()\n",
    "                image_tiles = image_tiles.view(-1, c, h, w)\n",
    "                mask_tiles = mask_tiles.view(-1, h, w)\n",
    "\n",
    "            image = image_tiles.to(device)\n",
    "            mask = mask_tiles.to(device)\n",
    "            # forward\n",
    "            output = model(image)\n",
    "            loss = criterion(output, mask)\n",
    "            # evaluation metrics\n",
    "            iou_score += mIoU(output, mask)\n",
    "            accuracy += pixel_accuracy(output, mask)\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()  # update weight\n",
    "            optimizer.zero_grad()  # reset gradient\n",
    "\n",
    "            # step the learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        else:\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            test_accuracy = 0\n",
    "            val_iou_score = 0\n",
    "            # validation loop\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(tqdm(val_loader)):\n",
    "                    # reshape to 9 patches from single image, delete batch size\n",
    "                    image_tiles, mask_tiles = data\n",
    "\n",
    "                    if patch:\n",
    "                        bs, n_tiles, c, h, w = image_tiles.size()\n",
    "                        image_tiles = image_tiles.view(-1, c, h, w)\n",
    "                        mask_tiles = mask_tiles.view(-1, h, w)\n",
    "\n",
    "                    image = image_tiles.to(device)\n",
    "                    mask = mask_tiles.to(device)\n",
    "                    output = model(image)\n",
    "                    # evaluation metrics\n",
    "                    val_iou_score += mIoU(output, mask)\n",
    "                    test_accuracy += pixel_accuracy(output, mask)\n",
    "                    # loss\n",
    "                    loss = criterion(output, mask)\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "            # calculation mean for each batch\n",
    "            train_losses.append(running_loss / len(train_loader))\n",
    "            test_losses.append(test_loss / len(val_loader))\n",
    "\n",
    "            if min_loss > (test_loss / len(val_loader)):\n",
    "                print(\n",
    "                    \"Loss Decreasing.. {:.3f} >> {:.3f} \".format(\n",
    "                        min_loss, (test_loss / len(val_loader))\n",
    "                    )\n",
    "                )\n",
    "                min_loss = test_loss / len(val_loader)\n",
    "                decrease += 1\n",
    "                if decrease % 5 == 0:\n",
    "                    print(\"saving model...\")\n",
    "                    torch.save(\n",
    "                        model,\n",
    "                        \"Unet-Mobilenet_v2_mIoU-{:.3f}.pt\".format(\n",
    "                            val_iou_score / len(val_loader)\n",
    "                        ),\n",
    "                    )\n",
    "\n",
    "            if (test_loss / len(val_loader)) > min_loss:\n",
    "                not_improve += 1\n",
    "                min_loss = test_loss / len(val_loader)\n",
    "                print(f\"Loss Not Decrease for {not_improve} time\")\n",
    "                if not_improve == 7:\n",
    "                    print(\"Loss not decrease for 7 times, Stop Training\")\n",
    "                    break\n",
    "\n",
    "            # iou\n",
    "            val_iou.append(val_iou_score / len(val_loader))\n",
    "            train_iou.append(iou_score / len(train_loader))\n",
    "            train_acc.append(accuracy / len(train_loader))\n",
    "            val_acc.append(test_accuracy / len(val_loader))\n",
    "            print(\n",
    "                \"Epoch:{}/{}..\".format(e + 1, epochs),\n",
    "                \"Train Loss: {:.3f}..\".format(running_loss / len(train_loader)),\n",
    "                \"Val Loss: {:.3f}..\".format(test_loss / len(val_loader)),\n",
    "                \"Train mIoU:{:.3f}..\".format(iou_score / len(train_loader)),\n",
    "                \"Val mIoU: {:.3f}..\".format(val_iou_score / len(val_loader)),\n",
    "                \"Train Acc:{:.3f}..\".format(accuracy / len(train_loader)),\n",
    "                \"Val Acc:{:.3f}..\".format(test_accuracy / len(val_loader)),\n",
    "                \"Time: {:.2f}m\".format((time.time() - since) / 60),\n",
    "            )\n",
    "\n",
    "        # Save checkpoint\n",
    "        history = {\n",
    "            \"train_loss\": train_losses,\n",
    "            \"val_loss\": test_losses,\n",
    "            \"train_miou\": train_iou,\n",
    "            \"val_miou\": val_iou,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"lrs\": lrs,\n",
    "        }\n",
    "        save_checkpoint(\n",
    "            model, optimizer, scheduler, e + 1, min_loss, history, checkpoint_dir\n",
    "        )\n",
    "\n",
    "    print(\"Total time: {:.2f} m\".format((time.time() - fit_time) / 60))\n",
    "    return history\n",
    "\n",
    "\n",
    "def predict_image_mask_miou(\n",
    "    model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "):\n",
    "    model.eval()\n",
    "    t = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize(mean, std)]\n",
    "    )  # Change this line\n",
    "    image = t(image)\n",
    "    model.to(device)\n",
    "    image = image.to(device)\n",
    "    mask = mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0)\n",
    "        mask = mask.unsqueeze(0)\n",
    "        output = model(image)\n",
    "        score = mIoU(output, mask)\n",
    "        masked = torch.argmax(output, dim=1)\n",
    "        masked = masked.cpu().squeeze(0)\n",
    "    return masked, score\n",
    "\n",
    "\n",
    "def predict_image_mask_pixel(\n",
    "    model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "):\n",
    "    model.eval()\n",
    "    t = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize(mean, std)]\n",
    "    )  # Change this line\n",
    "    image = t(image)\n",
    "    model.to(device)\n",
    "    image = image.to(device)\n",
    "    mask = mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0)\n",
    "        mask = mask.unsqueeze(0)\n",
    "        output = model(image)\n",
    "        acc = pixel_accuracy(output, mask)\n",
    "        masked = torch.argmax(output, dim=1)\n",
    "        masked = masked.cpu().squeeze(0)\n",
    "    return masked, acc\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_set):\n",
    "    score_iou = []\n",
    "    accuracy = []\n",
    "    for i in tqdm(range(len(test_set))):\n",
    "        img, mask = test_set[i]\n",
    "        pred_mask, score = predict_image_mask_miou(model, img, mask)\n",
    "        score_iou.append(score)\n",
    "        _, acc = predict_image_mask_pixel(model, img, mask)\n",
    "        accuracy.append(acc)\n",
    "    return np.mean(score_iou), np.mean(accuracy)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    loss = checkpoint[\"loss\"]\n",
    "    history = {\n",
    "        \"train_loss\": checkpoint[\"train_loss\"],\n",
    "        \"val_loss\": checkpoint[\"val_loss\"],\n",
    "        \"train_miou\": checkpoint[\"train_miou\"],\n",
    "        \"val_miou\": checkpoint[\"val_miou\"],\n",
    "        \"train_acc\": checkpoint[\"train_acc\"],\n",
    "        \"val_acc\": checkpoint[\"val_acc\"],\n",
    "        \"lrs\": checkpoint[\"lrs\"],\n",
    "    }\n",
    "    return model, optimizer, scheduler, epoch, loss, history\n",
    "\n",
    "\n",
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    checkpoints = [\n",
    "        f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint_epoch_\")\n",
    "    ]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    latest_checkpoint = max(\n",
    "        checkpoints, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0])\n",
    "    )\n",
    "    return os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "\n",
    "\n",
    "def resume_from_checkpoint(model, optimizer, scheduler, checkpoint_dir):\n",
    "    latest_checkpoint = get_latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        model, optimizer, scheduler, start_epoch, min_loss, history = load_checkpoint(\n",
    "            model, optimizer, scheduler, latest_checkpoint\n",
    "        )\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        return model, optimizer, scheduler, start_epoch, min_loss, history\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "        return model, optimizer, scheduler, 0, float(\"inf\"), {}\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, loss, history, checkpoint_dir):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"loss\": loss,\n",
    "        \"train_loss\": history.get(\"train_loss\", []),\n",
    "        \"val_loss\": history.get(\"val_loss\", []),\n",
    "        \"train_miou\": history.get(\"train_miou\", []),\n",
    "        \"val_miou\": history.get(\"val_miou\", []),\n",
    "        \"train_acc\": history.get(\"train_acc\", []),\n",
    "        \"val_acc\": history.get(\"val_acc\", []),\n",
    "        \"lrs\": history.get(\"lrs\", []),\n",
    "    }\n",
    "    torch.save(\n",
    "        checkpoint, os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"val_loss\"], label=\"val\", marker=\"o\")\n",
    "    plt.plot(history[\"train_loss\"], label=\"train\", marker=\"o\")\n",
    "    plt.title(\"Loss per epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_score(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"train_miou\"], label=\"train_mIoU\", marker=\"*\")\n",
    "    plt.plot(history[\"val_miou\"], label=\"val_mIoU\", marker=\"*\")\n",
    "    plt.title(\"Score per epoch\")\n",
    "    plt.ylabel(\"mean IoU\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_acc(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"train_acc\"], label=\"train_accuracy\", marker=\"*\")\n",
    "    plt.plot(history[\"val_acc\"], label=\"val_accuracy\", marker=\"*\")\n",
    "    plt.title(\"Accuracy per epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_predictions(\n",
    "    model,\n",
    "    test_set,\n",
    "    output_pdf,\n",
    "    num_classes=23,\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225],\n",
    "):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Create a colormap for the segmentation mask\n",
    "    cmap = plt.get_cmap(\"tab20\")\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, num_classes)]\n",
    "\n",
    "    with PdfPages(output_pdf) as pdf:\n",
    "        for i, (img, mask) in enumerate(test_set):\n",
    "            # Prepare the image\n",
    "            img_tensor = T.Compose([T.ToTensor(), T.Normalize(mean, std)])(img)\n",
    "            img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "            # Get the prediction\n",
    "            with torch.no_grad():\n",
    "                output = model(img_tensor)\n",
    "                pred_mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "            # Create a figure with three subplots\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            fig.suptitle(f\"Test Image {i+1}\")\n",
    "\n",
    "            # Plot original image\n",
    "            ax1.imshow(img)\n",
    "            ax1.set_title(\"Original Image\")\n",
    "            ax1.axis(\"off\")\n",
    "\n",
    "            # Plot ground truth mask\n",
    "            ax2.imshow(mask, cmap=cmap, vmin=0, vmax=num_classes - 1)\n",
    "            ax2.set_title(\"Ground Truth\")\n",
    "            ax2.axis(\"off\")\n",
    "\n",
    "            # Plot predicted mask\n",
    "            ax3.imshow(pred_mask, cmap=cmap, vmin=0, vmax=num_classes - 1)\n",
    "            ax3.set_title(\"Prediction\")\n",
    "            ax3.axis(\"off\")\n",
    "\n",
    "            # Add the plot to the PDF\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "\n",
    "        print(f\"Visualizations saved to {output_pdf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data directory exists.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup path to data folder\n",
    "# data_path = Path(\"data/\")\n",
    "# image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "DATA_PATH = Path(\"data/\")\n",
    "\n",
    "# If the image folder doesn't exist, download it and prepare it...\n",
    "if Path(DATA_PATH).is_dir():\n",
    "    print(f\"{DATA_PATH} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {DATA_PATH} directory, creating one...\")\n",
    "    Path(DATA_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Download pizza, steak, sushi data\n",
    "    with open(DATA_PATH / \"archive.zip\", \"wb\") as f:\n",
    "        # request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "        request = requests.get(\"https://storage.googleapis.com/kaggle-data-sets/333968/1834160/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20241023%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20241023T113757Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=3e3bbcf7cb80c007d26471d7f7be115d075367ab5a6e241b83823607ac7683cb813a1757c5e71ee5052498b967758686f92be595272f684bf1a90bd8a21681ba7ba7a34e074464ac5e2d3b944af4ebf34d425d50281034b3fd3c17f1f15320f27eaf578cfbead4c6e40b721f1209333e55c6185b157001d9afd3762fd3f6eadb67ee4841ba059b999775c14615537f31e44b0f3e2cea010e3c13b612d18d952cf22c7d101962cdefe0da4d4e6a03345f9d3ceb14048de01e987345e318361b9d2f8cea7c9fb749de9c78eea4795da2e71ae5d8e065206627970bebb1eb523d7cf03d413978eb542f3f0500538b53bda10f198ca97f5f85d267bbe40b269487dd\")\n",
    "        print(\"Downloading drone dataset ...\")\n",
    "        f.write(request.content)\n",
    "\n",
    "    # Unzip pizza, steak, sushi data\n",
    "    with zipfile.ZipFile(DATA_PATH / \"archive.zip\", \"r\") as zip_ref:\n",
    "        print(\"Unzipping drone dataset ...\")\n",
    "        zip_ref.extractall(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Starting from scratch.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from cnn_data import get_data_loaders\n",
    "# from model import create_model, fit, evaluate_model, resume_from_checkpoint\n",
    "# from viz import plot_loss, plot_score, plot_acc, visualize_predictions\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_set = get_data_loaders(batch_size=16)\n",
    "\n",
    "# Create model\n",
    "model = create_model()\n",
    "\n",
    "# Training parameters\n",
    "max_lr = 1e-3\n",
    "epochs = 4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader)\n",
    ")\n",
    "\n",
    "# Check for existing checkpoints and resume training if possible\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "model, optimizer, sched, start_epoch, min_loss, history = resume_from_checkpoint(\n",
    "    model, optimizer, sched, checkpoint_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 20/20 [02:02<00:00,  6.14s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 4/4 [00:18<00:00,  4.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. inf >> 5.935 \n",
      "Epoch:1/4.. Train Loss: 2.741.. Val Loss: 5.935.. Train mIoU:0.048.. Val mIoU: 0.076.. Train Acc:0.272.. Val Acc:0.496.. Time: 2.36m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 1/20 [00:11<03:41, 11.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnet-Mobilenet.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[52], line 68\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, patch, checkpoint_dir)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[1;32m     67\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 68\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# training phase\u001b[39;49;00m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_tiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_tiles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpatch\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml311/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml311/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/ml311/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[50], line 47\u001b[0m, in \u001b[0;36mDroneDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 47\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     49\u001b[0m     mask \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX[idx] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "history = fit(\n",
    "    epochs - start_epoch,\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    sched,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model, \"Unet-Mobilenet.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "plot_loss(history)\n",
    "plot_score(history)\n",
    "plot_acc(history)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_miou, test_accuracy = evaluate_model(model, test_set)\n",
    "print(\"Test Set mIoU:\", test_miou)\n",
    "print(\"Test Set Pixel Accuracy:\", test_accuracy)\n",
    "\n",
    "# Visualize predictions\n",
    "visualize_predictions(model, test_set, \"test_predictions.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
