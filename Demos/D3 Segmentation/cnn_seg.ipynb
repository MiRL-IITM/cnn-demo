{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Image Segmentation Tutorial\n",
    "\n",
    "## Table of Contents\n",
    "- [CNN Image Segmentation Tutorial](#cnn-image-segmentation-tutorial)\n",
    "  - [Table of Contents](#table-of-contents)\n",
    "  - [Introduction](#introduction)\n",
    "    - [What is Image Segmentation?](#what-is-image-segmentation)\n",
    "    - [Types of Segmentation](#types-of-segmentation)\n",
    "    - [Real-world Applications](#real-world-applications)\n",
    "  - [Prerequisites](#prerequisites)\n",
    "    - [Required Libraries](#required-libraries)\n",
    "    - [Dataset Overview](#dataset-overview)\n",
    "  - [Data Preparation](#data-preparation)\n",
    "    - [Dataset Organization](#dataset-organization)\n",
    "    - [Custom Dataset Classes](#custom-dataset-classes)\n",
    "    - [Data Preprocessing](#data-preprocessing)\n",
    "    - [Data Augmentation](#data-augmentation)\n",
    "    - [DataLoader Creation](#dataloader-creation)\n",
    "  - [Model Architecture](#model-architecture)\n",
    "    - [U-Net with MobileNetV2](#u-net-with-mobilenetv2)\n",
    "      - [Architecture Components:](#architecture-components)\n",
    "    - [Training Components](#training-components)\n",
    "      - [Loss Function](#loss-function)\n",
    "      - [Optimizer](#optimizer)\n",
    "      - [Learning Rate Scheduler](#learning-rate-scheduler)\n",
    "    - [Model Configuration](#model-configuration)\n",
    "    - [Device Management](#device-management)\n",
    "  - [Metrics (`metrics.py`)](#metrics-metricspy)\n",
    "    - [Pixel Accuracy](#pixel-accuracy)\n",
    "    - [Mean IoU (Intersection over Union)](#mean-iou-intersection-over-union)\n",
    "  - [Training and Inference (`model.py`)](#training-and-inference-modelpy)\n",
    "    - [Training Pipeline](#training-pipeline)\n",
    "    - [Inference Pipeline](#inference-pipeline)\n",
    "  - [Visualization (`viz.py`)](#visualization-vizpy)\n",
    "    - [Training Progress Visualization](#training-progress-visualization)\n",
    "    - [Prediction Visualization](#prediction-visualization)\n",
    "  - [Downloading the Dataset](#downloading-the-dataset)\n",
    "  - [Main Training Script](#main-training-script)\n",
    "  - [Main Inference Script](#main-inference-script)\n",
    "  - [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Image segmentation is a fundamental computer vision task that involves dividing an\n",
    "image into multiple segments or regions, where each pixel in the image is assigned\n",
    "a class label. Unlike classification, which provides a single label for an entire\n",
    "image, or object detection, which identifies object locations with bounding boxes,\n",
    "segmentation provides pixel-level understanding of the image content.\n",
    "\n",
    "In this tutorial, we'll be working with semantic segmentation, where we assign\n",
    "each pixel to a predefined class category. For example, in our drone imagery\n",
    "dataset, pixels might belong to classes such as 'building', 'vegetation', 'road',\n",
    "or 'vehicle'.\n",
    "\n",
    "### Types of Segmentation\n",
    "There are several types of image segmentation, each serving different purposes:\n",
    "\n",
    "1. **Semantic Segmentation**\n",
    "   - Assigns each pixel to a class category\n",
    "   - Doesn't distinguish between instances of the same class\n",
    "   - Example: All 'car' pixels get the same label, regardless of how many cars\n",
    "     are present\n",
    "\n",
    "2. **Instance Segmentation**\n",
    "   - Identifies individual instances of objects\n",
    "   - Distinguishes between different instances of the same class\n",
    "   - Example: Each car in the image gets a unique instance ID\n",
    "\n",
    "3. **Panoptic Segmentation**\n",
    "   - Combines semantic segmentation for background classes\n",
    "   - Adds instance segmentation for countable objects\n",
    "   - Provides a unified view of scene understanding\n",
    "\n",
    "### Real-world Applications\n",
    "Image segmentation has numerous practical applications across various fields:\n",
    "\n",
    "1. **Aerial and Satellite Imagery**\n",
    "   - Urban planning and development\n",
    "   - Agricultural monitoring\n",
    "   - Disaster response and damage assessment\n",
    "   - Environmental monitoring\n",
    "\n",
    "2. **Medical Imaging**\n",
    "   - Tumor detection and measurement\n",
    "   - Organ segmentation\n",
    "   - Cell counting and analysis\n",
    "   - Surgical planning\n",
    "\n",
    "3. **Autonomous Vehicles**\n",
    "   - Road and lane detection\n",
    "   - Obstacle identification\n",
    "   - Pedestrian segmentation\n",
    "   - Traffic analysis\n",
    "\n",
    "4. **Industrial Applications**\n",
    "   - Quality control and inspection\n",
    "\n",
    "\n",
    "In this tutorial, we'll focus on semantic segmentation of aerial drone imagery,\n",
    "which has important applications in urban planning, mapping, and environmental\n",
    "monitoring. Our implementation uses a U-Net architecture with a MobileNetV2\n",
    "backbone, providing an efficient and effective solution for real-world\n",
    "segmentation tasks.\n",
    "\n",
    "\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "We'll be working with the Semantic Drone Dataset, which consists of:\n",
    "- Aerial imagery captured by drones\n",
    "- High-resolution RGB images (6000x4000px)\n",
    "- Pixel-wise semantic segmentation masks\n",
    "- 23 different class categories including:\n",
    "  - Buildings\n",
    "  - Roads\n",
    "  - Vegetation\n",
    "  - Vehicles\n",
    "  - People\n",
    "  - Other urban features\n",
    "\n",
    "![Semantic Drone Dataset Example](https://www.kaggleusercontent.com/kf/55737032/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..x_-exPHMZnlgjHP-2ibtug.GSbJPnhFB0nT3LkxG2ODuIkwkAO44uhpRSDSvSDWsX_JVxZML6hT1QcRVhiknaMFNUgzfshyJ0oLQdRlSP92On2e9Nzv8Zxo2BNJuWNvfINFHXb_1R1QJbVsK8HPJgF9tgEUOaudVM9fvJCDvC8Id2lw7vBiXGPdWRzF8r7xDDMzk-jX-vL5SVOBTXc4UgYltiwEbRatnykFZ8LEq03j2vb7hb5XhkgCXAPl0FOkrpJGuxKwKHSSzyviZqO3g9dP2_kUk4_bfzIbKn-hek3k3MNHxjZhEfrEmcNkgLBzoLuhbxVDRYd7tyheAyga3UA3qggzBE3YDIDJUJgWE3Qf3uzoPL5o2DzB2PirGapRoE387CjR8tZzxnrC3H1c6nYNS7UcByrOnpLgszotGjNoAwFT_l9UpQMblXw_ln2MP1Cqw3o0Ab1JhasCIS7kbY9VurenuulbpwEJH6QJDeliLO9Q3tUvvqPITYjyvjxTgOb8kSF9X8Q93KpJKbRFGO_KyelKe0Yb1ak8fjyP-m0HiXcfnjXEb0Z0aGhFQ4jTv1CwpiBSwxtoPGNVgzm_xb50lyUgXRFxCsTSPUR-BDT9w8q4KephJSoDuZy-Jb43pZaHH2r1fTh6gWnbFAJJVXC_IW0v5a1t8ZLbaAqL8C-E7sh3tJUJwGfmdXLNdVJ3kgQ.6S4uje2IxtNyDT9Db6YPww/__results___files/__results___33_0.png)\n",
    "\n",
    "The dataset structure:\n",
    "```\n",
    "data/\n",
    "├── dataset/\n",
    "│   └── semantic_drone_dataset/\n",
    "│       ├── original_images/     # RGB images\n",
    "│       └── label_images_semantic/  # Segmentation masks\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before diving into the implementation, let's review the essential libraries and tools we'll be using in this project.\n",
    "\n",
    "### Required Libraries\n",
    "\n",
    "1. **PyTorch**\n",
    "   - Deep learning framework\n",
    "   - Provides neural network building blocks and GPU acceleration\n",
    "   - Version requirement: >= 1.7.0\n",
    "\n",
    "2. **OpenCV (cv2)**\n",
    "   - Image loading and preprocessing\n",
    "   - Color space conversions\n",
    "   - Basic image operations\n",
    "   ```python\n",
    "   pip install opencv-python\n",
    "   ```\n",
    "\n",
    "3. **Albumentations**\n",
    "   - High-performance data augmentation\n",
    "   - Implements various image transformations\n",
    "   - Specially designed for segmentation tasks\n",
    "   ```python\n",
    "   pip install albumentations\n",
    "   ```\n",
    "\n",
    "4. **Segmentation Models PyTorch (smp)**\n",
    "   - Pre-implemented segmentation architectures\n",
    "   - Provides pre-trained encoders\n",
    "   - Easy-to-use interface for segmentation models\n",
    "   ```python\n",
    "   pip install segmentation-models-pytorch\n",
    "   ```\n",
    "\n",
    "5. **Supporting Libraries**\n",
    "   - NumPy: Numerical operations and array manipulation\n",
    "   - Pandas: Data organization and splitting\n",
    "   - Matplotlib: Visualization and result plotting\n",
    "   - tqdm: Progress bar for training loops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preparation\n",
    "\n",
    "The data preparation pipeline is implemented in `cnn_data.py` and handles dataset organization, loading, and preprocessing.\n",
    "\n",
    "### Dataset Organization\n",
    "\n",
    "First, we define the paths to our image and mask directories:\n",
    "\n",
    "```python\n",
    "IMAGE_PATH = \"data/dataset/semantic_drone_dataset/original_images/\"\n",
    "MASK_PATH = \"data/dataset/semantic_drone_dataset/label_images_semantic/\"\n",
    "```\n",
    "\n",
    "We create a DataFrame to organize our data and split it into training, validation, and test sets:\n",
    "\n",
    "```python\n",
    "def create_df():\n",
    "    name = []\n",
    "    for dirname, _, filenames in os.walk(IMAGE_PATH):\n",
    "        for filename in filenames:\n",
    "            name.append(filename.split(\".\")[0])\n",
    "    return pd.DataFrame({\"id\": name}, index=np.arange(0, len(name)))\n",
    "\n",
    "def get_data_splits():\n",
    "    df = create_df()\n",
    "    # Split: 75% train, 15% validation, 10% test\n",
    "    X_trainval, X_test = train_test_split(df[\"id\"].values, test_size=0.1, random_state=19)\n",
    "    X_train, X_val = train_test_split(X_trainval, test_size=0.15, random_state=19)\n",
    "    return X_train, X_val, X_test\n",
    "```\n",
    "\n",
    "### Custom Dataset Classes\n",
    "\n",
    "We implement two custom PyTorch Dataset classes:\n",
    "\n",
    "1. **DroneDataset**: For training and validation data\n",
    "```python\n",
    "class DroneDataset(Dataset):\n",
    "    def __init__(self, img_path, mask_path, X, mean, std, transform=None, patch=False):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "        self.patches = patch\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        img = cv2.imread(self.img_path + self.X[idx] + \".jpg\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_path + self.X[idx] + \".png\",\n",
    "                         cv2.IMREAD_GRAYSCALE)\n",
    "```\n",
    "\n",
    "2. **DroneTestDataset**: Specialized for test data with simplified processing\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "Our preprocessing pipeline includes:\n",
    "\n",
    "1. **Image Loading**\n",
    "   - Read images using OpenCV\n",
    "   - Convert from BGR to RGB color space\n",
    "   - Load masks as grayscale images\n",
    "\n",
    "2. **Normalization**\n",
    "   - Standardize images using ImageNet statistics:\n",
    "   ```python\n",
    "   mean = [0.485, 0.456, 0.406]\n",
    "   std = [0.229, 0.224, 0.225]\n",
    "   ```\n",
    "\n",
    "3. **Tensor Conversion**\n",
    "   - Convert images to PyTorch tensors\n",
    "   - Convert masks to long tensors for classification\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "We use the Albumentations library for efficient data augmentation:\n",
    "\n",
    "```python\n",
    "t_train = A.Compose([\n",
    "    A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST),\n",
    "    A.HorizontalFlip(),\n",
    "    A.VerticalFlip(),\n",
    "    A.GridDistortion(p=0.2),\n",
    "    A.RandomBrightnessContrast((0, 0.5), (0, 0.5)),\n",
    "    A.GaussNoise(),\n",
    "])\n",
    "\n",
    "t_val = A.Compose([\n",
    "    A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST),\n",
    "    A.HorizontalFlip(),\n",
    "    A.GridDistortion(p=0.2),\n",
    "])\n",
    "```\n",
    "\n",
    "The augmentation pipeline includes:\n",
    "- Resizing to a consistent size\n",
    "- Random horizontal and vertical flips\n",
    "- Grid distortion for geometric variety\n",
    "- Brightness and contrast adjustments\n",
    "- Gaussian noise for robustness\n",
    "\n",
    "### DataLoader Creation\n",
    "\n",
    "Finally, we create PyTorch DataLoaders for efficient batch processing:\n",
    "\n",
    "```python\n",
    "def get_data_loaders(batch_size=16):\n",
    "    X_train, X_val, X_test = get_data_splits()\n",
    "\n",
    "    train_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_train,\n",
    "                            mean, std, t_train, patch=False)\n",
    "    val_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_val,\n",
    "                          mean, std, t_val, patch=False)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, val_loader, test_set\n",
    "```\n",
    "\n",
    "This setup provides:\n",
    "- Batched data loading\n",
    "- Automatic shuffling\n",
    "- Parallel data loading capabilities\n",
    "- Memory-efficient data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/ml311/lib/python3.11/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.20 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "\n",
    "IMAGE_PATH = \"data/dataset/semantic_drone_dataset/original_images/\"\n",
    "MASK_PATH = \"data/dataset/semantic_drone_dataset/label_images_semantic/\"\n",
    "\n",
    "\n",
    "def create_df():\n",
    "    name = []\n",
    "    for dirname, _, filenames in os.walk(IMAGE_PATH):\n",
    "        for filename in filenames:\n",
    "            name.append(filename.split(\".\")[0])\n",
    "    return pd.DataFrame({\"id\": name}, index=np.arange(0, len(name)))\n",
    "\n",
    "\n",
    "def get_data_splits():\n",
    "    df = create_df()\n",
    "    X_trainval, X_test = train_test_split(\n",
    "        df[\"id\"].values, test_size=0.1, random_state=19\n",
    "    )\n",
    "    X_train, X_val = train_test_split(X_trainval, test_size=0.15, random_state=19)\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "\n",
    "class DroneDataset(Dataset):\n",
    "    def __init__(self, img_path, mask_path, X, mean, std, transform=None, patch=False):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "        self.patches = patch\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_path + self.X[idx] + \".jpg\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_path + self.X[idx] + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img, mask=mask)\n",
    "            img = Image.fromarray(aug[\"image\"])\n",
    "            mask = aug[\"mask\"]\n",
    "\n",
    "        if self.transform is None:\n",
    "            img = Image.fromarray(img)\n",
    "\n",
    "        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n",
    "        img = t(img)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        if self.patches:\n",
    "            img, mask = self.tiles(img, mask)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "    def tiles(self, img, mask):\n",
    "        img_patches = img.unfold(1, 512, 512).unfold(2, 768, 768)\n",
    "        img_patches = img_patches.contiguous().view(3, -1, 512, 768)\n",
    "        img_patches = img_patches.permute(1, 0, 2, 3)\n",
    "\n",
    "        mask_patches = mask.unfold(0, 512, 512).unfold(1, 768, 768)\n",
    "        mask_patches = mask_patches.contiguous().view(-1, 512, 768)\n",
    "\n",
    "        return img_patches, mask_patches\n",
    "\n",
    "\n",
    "class DroneTestDataset(Dataset):\n",
    "    def __init__(self, img_path, mask_path, X, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_path + self.X[idx] + \".jpg\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_path + self.X[idx] + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img, mask=mask)\n",
    "            img = Image.fromarray(aug[\"image\"])\n",
    "            mask = aug[\"mask\"]\n",
    "\n",
    "        if self.transform is None:\n",
    "            img = Image.fromarray(img)\n",
    "\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "def get_data_loaders(batch_size=16):\n",
    "    X_train, X_val, X_test = get_data_splits()\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    t_train = A.Compose(\n",
    "        [\n",
    "            A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST),\n",
    "            A.HorizontalFlip(),\n",
    "            A.VerticalFlip(),\n",
    "            A.GridDistortion(p=0.2),\n",
    "            A.RandomBrightnessContrast((0, 0.5), (0, 0.5)),\n",
    "            A.GaussNoise(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    t_val = A.Compose(\n",
    "        [\n",
    "            A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST),\n",
    "            A.HorizontalFlip(),\n",
    "            A.GridDistortion(p=0.2),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_set = DroneDataset(\n",
    "        IMAGE_PATH, MASK_PATH, X_train, mean, std, t_train, patch=False\n",
    "    )\n",
    "    val_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_val, mean, std, t_val, patch=False)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    t_test = A.Resize(768, 1152, interpolation=cv2.INTER_NEAREST)\n",
    "    test_set = DroneTestDataset(IMAGE_PATH, MASK_PATH, X_test, transform=t_test)\n",
    "\n",
    "    return train_loader, val_loader, test_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics (`metrics.py`)\n",
    "\n",
    "Our implementation uses two key metrics to evaluate segmentation performance:\n",
    "\n",
    "### Pixel Accuracy\n",
    "```python\n",
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy\n",
    "```\n",
    "\n",
    "- Measures percentage of correctly classified pixels\n",
    "- Simple but potentially misleading for imbalanced classes\n",
    "- Range: [0, 1], where 1 is perfect classification\n",
    "\n",
    "### Mean IoU (Intersection over Union)\n",
    "```python\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes):\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0:\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union + smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)\n",
    "```\n",
    "\n",
    "- Calculates intersection over union for each class\n",
    "- Better metric for imbalanced datasets\n",
    "- Handles missing classes with NaN values\n",
    "- Range: [0, 1], where 1 is perfect segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes):  # loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0:  # no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = (\n",
    "                    torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                )\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union + smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "Our implementation uses a U-Net architecture with a MobileNetV2 backbone, leveraging the segmentation-models-pytorch (smp) library for efficient implementation.\n",
    "\n",
    "### U-Net with MobileNetV2\n",
    "\n",
    "The model is created using the following configuration:\n",
    "\n",
    "```python\n",
    "def create_model():\n",
    "    return smp.Unet(\n",
    "        \"mobilenet_v2\",          # Encoder backbone\n",
    "        encoder_weights=\"imagenet\",  # Pre-trained weights\n",
    "        classes=23,              # Number of output classes\n",
    "        activation=None,         # No activation (handled by loss function)\n",
    "        encoder_depth=5,         # Number of encoder stages\n",
    "        decoder_channels=[256, 128, 64, 32, 16]  # Decoder channel sizes\n",
    "    )\n",
    "```\n",
    "\n",
    "#### Architecture Components:\n",
    "\n",
    "1. **Encoder (MobileNetV2)**\n",
    "   - Pre-trained on ImageNet\n",
    "   - Efficient mobile-first architecture\n",
    "   - Feature extraction at multiple scales\n",
    "   - 5 stages of downsampling\n",
    "\n",
    "2. **Decoder**\n",
    "   - Progressive upsampling path\n",
    "   - Channel sizes: [256, 128, 64, 32, 16]\n",
    "   - Skip connections from encoder\n",
    "   - Feature refinement at each stage\n",
    "\n",
    "3. **Output Layer**\n",
    "   - 23 output channels (one per class)\n",
    "   - No activation (handled by CrossEntropyLoss)\n",
    "\n",
    "### Training Components\n",
    "\n",
    "#### Loss Function\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "```\n",
    "- Combines LogSoftmax and NLLLoss\n",
    "- Handles multi-class segmentation\n",
    "- Automatically normalizes class predictions\n",
    "\n",
    "#### Optimizer\n",
    "```python\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=max_lr,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "```\n",
    "- AdamW optimizer for better generalization\n",
    "- Weight decay for regularization\n",
    "- Adaptive learning rate adjustments\n",
    "\n",
    "#### Learning Rate Scheduler\n",
    "```python\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=len(train_loader)\n",
    ")\n",
    "```\n",
    "- One Cycle learning rate policy\n",
    "- Gradual warmup and cooldown\n",
    "- Helps prevent overfitting\n",
    "- Improves training stability\n",
    "\n",
    "### Model Configuration\n",
    "\n",
    "Key hyperparameters for the model:\n",
    "```python\n",
    "max_lr = 1e-3        # Maximum learning rate\n",
    "epochs = 30          # Number of training epochs\n",
    "weight_decay = 1e-4  # L2 regularization factor\n",
    "batch_size = 16      # Images per batch\n",
    "```\n",
    "\n",
    "### Device Management\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move model to GPU if available\n",
    "```\n",
    "\n",
    "This architecture provides:\n",
    "- Efficient feature extraction through MobileNetV2\n",
    "- High-resolution detail through skip connections\n",
    "- Memory-efficient training\n",
    "- Fast inference capabilities\n",
    "- Good balance of accuracy and computational cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture (U-Net) More Details\n",
    "\n",
    "U-Net Architecture for Image Segmentation\n",
    "\n",
    "![U-Net Architecture](https://media.geeksforgeeks.org/wp-content/uploads/20220614121231/Group14.jpg)\n",
    "\n",
    "The U-Net architecture is a powerful neural network design specifically crafted for precise image segmentation tasks. Its distinctive U-shaped structure consists of two main paths:\n",
    "\n",
    "Contracting Path (Encoder):\n",
    "- Begins with the input image and progressively reduces spatial dimensions\n",
    "- Uses consecutive convolutional layers followed by max pooling operations\n",
    "- Each downsampling step doubles the number of feature channels\n",
    "- Captures increasingly abstract features and broader contextual information\n",
    "- Final encoded representation contains high-level semantic information\n",
    "\n",
    "Expanding Path (Decoder) with Up-Convolutions:\n",
    "- Up-convolutions (transposed convolutions) gradually restore spatial dimensions\n",
    "- Each up-convolution halves the number of feature channels\n",
    "- Transforms low-resolution, abstract features back into detailed spatial information\n",
    "- Enables the network to generate full-resolution segmentation maps\n",
    "- Learns to reconstruct spatial details from compressed representations\n",
    "\n",
    "Skip Connections - The Critical Bridge:\n",
    "- Direct connections between corresponding encoder and decoder layers\n",
    "- Concatenates high-resolution features from encoder with upsampled features\n",
    "- Preserves fine spatial details that would otherwise be lost in compression\n",
    "- Helps combat the vanishing gradient problem during training\n",
    "- Enables precise boundary detection in segmentation masks\n",
    "\n",
    "The architecture excels at segmentation because:\n",
    "1. Multi-scale feature processing captures both fine details and global context\n",
    "2. Skip connections maintain spatial precision throughout the network\n",
    "3. Gradual upsampling allows the network to learn optimal feature reconstruction\n",
    "4. The symmetric structure balances feature extraction and reconstruction\n",
    "\n",
    "In our implementation with MobileNetV2:\n",
    "- The efficient MobileNetV2 backbone serves as the encoder\n",
    "- Custom decoder layers mirror the encoder's structure\n",
    "- Multiple skip connections at different scales preserve spatial information\n",
    "- Final output layer produces per-pixel class predictions\n",
    "- Architecture optimized for both accuracy and computational efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is upconvolution?\n",
    "\n",
    "Upconvolution (also called transposed convolution or deconvolution) is essentially the reverse operation of traditional convolution.\n",
    "\n",
    "While regular convolution reduces spatial dimensions by sliding a kernel over the input:\n",
    "  e.g., 4x4 input -> 2x2 output with 3x3 kernel and stride 2\n",
    "\n",
    "Upconvolution increases spatial dimensions by:\n",
    "1. Inserting zeros between input elements (based on stride)\n",
    "2. Performing regular convolution with a learnable kernel\n",
    "3. Producing a larger output\n",
    "  e.g., 2x2 input -> 4x4 output with 3x3 kernel and stride 2\n",
    "\n",
    "Key differences from regular convolution:\n",
    "- Increases spatial dimensions instead of reducing them\n",
    "- Still uses learnable kernels but applies them differently\n",
    "- Often used in decoders to restore resolution lost in encoding\n",
    "\n",
    "The name \"deconvolution\" is technically incorrect since it's not truly inverting convolution,\n",
    "but rather learning an upsampling transformation that best reconstructs the desired output resolution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training and Inference (`model.py`)\n",
    "\n",
    "### Training Pipeline\n",
    "```python\n",
    "def train_model(train_loader, val_loader, epochs=30, checkpoint_dir=\"checkpoints\"):\n",
    "    model = create_model()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n",
    "                                                  steps_per_epoch=len(train_loader))\n",
    "\n",
    "    history = fit(epochs, model, train_loader, val_loader, criterion,\n",
    "                 optimizer, scheduler, checkpoint_dir=checkpoint_dir)\n",
    "    return model, history\n",
    "```\n",
    "\n",
    "Key components:\n",
    "- Model creation with MobileNetV2 backbone\n",
    "- CrossEntropyLoss for multi-class segmentation\n",
    "- AdamW optimizer with OneCycleLR scheduler\n",
    "- Checkpoint management for model saving/loading\n",
    "\n",
    "### Inference Pipeline\n",
    "```python\n",
    "def inference(model_path, image_path):\n",
    "    model = create_model()\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image = load_image(image_path)\n",
    "        prediction = model(image.unsqueeze(0))\n",
    "        prediction = torch.argmax(prediction.squeeze(), dim=0)\n",
    "    return prediction\n",
    "```\n",
    "\n",
    "Features:\n",
    "- Model loading from checkpoint\n",
    "- Single image prediction\n",
    "- No gradient computation during inference\n",
    "- Returns class predictions per pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from metrics import mIoU, pixel_accuracy\n",
    "import os\n",
    "from torchvision import transforms  # Change this line\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    return smp.Unet(\n",
    "        \"mobilenet_v2\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        classes=23,\n",
    "        activation=None,\n",
    "        encoder_depth=5,\n",
    "        decoder_channels=[256, 128, 64, 32, 16],\n",
    "    )\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "\n",
    "def fit(\n",
    "    epochs,\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    patch=False,\n",
    "    checkpoint_dir=\"checkpoints\",\n",
    "):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_iou = []\n",
    "    val_acc = []\n",
    "    train_iou = []\n",
    "    train_acc = []\n",
    "    lrs = []\n",
    "    min_loss = np.inf\n",
    "    decrease = 1\n",
    "    not_improve = 0\n",
    "\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    model.to(device)\n",
    "    fit_time = time.time()\n",
    "    for e in range(epochs):\n",
    "        since = time.time()\n",
    "        running_loss = 0\n",
    "        iou_score = 0\n",
    "        accuracy = 0\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for i, data in enumerate(tqdm(train_loader)):\n",
    "            # training phase\n",
    "            image_tiles, mask_tiles = data\n",
    "            if patch:\n",
    "                bs, n_tiles, c, h, w = image_tiles.size()\n",
    "                image_tiles = image_tiles.view(-1, c, h, w)\n",
    "                mask_tiles = mask_tiles.view(-1, h, w)\n",
    "\n",
    "            image = image_tiles.to(device)\n",
    "            mask = mask_tiles.to(device)\n",
    "            # forward\n",
    "            output = model(image)\n",
    "            loss = criterion(output, mask)\n",
    "            # evaluation metrics\n",
    "            iou_score += mIoU(output, mask)\n",
    "            accuracy += pixel_accuracy(output, mask)\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()  # update weight\n",
    "            optimizer.zero_grad()  # reset gradient\n",
    "\n",
    "            # step the learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        else:\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            test_accuracy = 0\n",
    "            val_iou_score = 0\n",
    "            # validation loop\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(tqdm(val_loader)):\n",
    "                    # reshape to 9 patches from single image, delete batch size\n",
    "                    image_tiles, mask_tiles = data\n",
    "\n",
    "                    if patch:\n",
    "                        bs, n_tiles, c, h, w = image_tiles.size()\n",
    "                        image_tiles = image_tiles.view(-1, c, h, w)\n",
    "                        mask_tiles = mask_tiles.view(-1, h, w)\n",
    "\n",
    "                    image = image_tiles.to(device)\n",
    "                    mask = mask_tiles.to(device)\n",
    "                    output = model(image)\n",
    "                    # evaluation metrics\n",
    "                    val_iou_score += mIoU(output, mask)\n",
    "                    test_accuracy += pixel_accuracy(output, mask)\n",
    "                    # loss\n",
    "                    loss = criterion(output, mask)\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "            # calculation mean for each batch\n",
    "            train_losses.append(running_loss / len(train_loader))\n",
    "            test_losses.append(test_loss / len(val_loader))\n",
    "\n",
    "            if min_loss > (test_loss / len(val_loader)):\n",
    "                print(\n",
    "                    \"Loss Decreasing.. {:.3f} >> {:.3f} \".format(\n",
    "                        min_loss, (test_loss / len(val_loader))\n",
    "                    )\n",
    "                )\n",
    "                min_loss = test_loss / len(val_loader)\n",
    "                decrease += 1\n",
    "                if decrease % 5 == 0:\n",
    "                    print(\"saving model...\")\n",
    "                    torch.save(\n",
    "                        model,\n",
    "                        \"Unet-Mobilenet_v2_mIoU-{:.3f}.pt\".format(\n",
    "                            val_iou_score / len(val_loader)\n",
    "                        ),\n",
    "                    )\n",
    "\n",
    "            if (test_loss / len(val_loader)) > min_loss:\n",
    "                not_improve += 1\n",
    "                min_loss = test_loss / len(val_loader)\n",
    "                print(f\"Loss Not Decrease for {not_improve} time\")\n",
    "                if not_improve == 7:\n",
    "                    print(\"Loss not decrease for 7 times, Stop Training\")\n",
    "                    break\n",
    "\n",
    "            # iou\n",
    "            val_iou.append(val_iou_score / len(val_loader))\n",
    "            train_iou.append(iou_score / len(train_loader))\n",
    "            train_acc.append(accuracy / len(train_loader))\n",
    "            val_acc.append(test_accuracy / len(val_loader))\n",
    "            print(\n",
    "                \"Epoch:{}/{}..\".format(e + 1, epochs),\n",
    "                \"Train Loss: {:.3f}..\".format(running_loss / len(train_loader)),\n",
    "                \"Val Loss: {:.3f}..\".format(test_loss / len(val_loader)),\n",
    "                \"Train mIoU:{:.3f}..\".format(iou_score / len(train_loader)),\n",
    "                \"Val mIoU: {:.3f}..\".format(val_iou_score / len(val_loader)),\n",
    "                \"Train Acc:{:.3f}..\".format(accuracy / len(train_loader)),\n",
    "                \"Val Acc:{:.3f}..\".format(test_accuracy / len(val_loader)),\n",
    "                \"Time: {:.2f}m\".format((time.time() - since) / 60),\n",
    "            )\n",
    "\n",
    "        # Save checkpoint\n",
    "        history = {\n",
    "            \"train_loss\": train_losses,\n",
    "            \"val_loss\": test_losses,\n",
    "            \"train_miou\": train_iou,\n",
    "            \"val_miou\": val_iou,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"lrs\": lrs,\n",
    "        }\n",
    "        save_checkpoint(\n",
    "            model, optimizer, scheduler, e + 1, min_loss, history, checkpoint_dir\n",
    "        )\n",
    "\n",
    "    print(\"Total time: {:.2f} m\".format((time.time() - fit_time) / 60))\n",
    "    return history\n",
    "\n",
    "\n",
    "def predict_image_mask_miou(\n",
    "    model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "):\n",
    "    model.eval()\n",
    "    t = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize(mean, std)]\n",
    "    )  # Change this line\n",
    "    image = t(image)\n",
    "    model.to(device)\n",
    "    image = image.to(device)\n",
    "    mask = mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0)\n",
    "        mask = mask.unsqueeze(0)\n",
    "        output = model(image)\n",
    "        score = mIoU(output, mask)\n",
    "        masked = torch.argmax(output, dim=1)\n",
    "        masked = masked.cpu().squeeze(0)\n",
    "    return masked, score\n",
    "\n",
    "\n",
    "def predict_image_mask_pixel(\n",
    "    model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "):\n",
    "    model.eval()\n",
    "    t = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize(mean, std)]\n",
    "    )  # Change this line\n",
    "    image = t(image)\n",
    "    model.to(device)\n",
    "    image = image.to(device)\n",
    "    mask = mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0)\n",
    "        mask = mask.unsqueeze(0)\n",
    "        output = model(image)\n",
    "        acc = pixel_accuracy(output, mask)\n",
    "        masked = torch.argmax(output, dim=1)\n",
    "        masked = masked.cpu().squeeze(0)\n",
    "    return masked, acc\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_set):\n",
    "    score_iou = []\n",
    "    accuracy = []\n",
    "    for i in tqdm(range(len(test_set))):\n",
    "        img, mask = test_set[i]\n",
    "        pred_mask, score = predict_image_mask_miou(model, img, mask)\n",
    "        score_iou.append(score)\n",
    "        _, acc = predict_image_mask_pixel(model, img, mask)\n",
    "        accuracy.append(acc)\n",
    "    return np.mean(score_iou), np.mean(accuracy)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    loss = checkpoint[\"loss\"]\n",
    "    history = {\n",
    "        \"train_loss\": checkpoint[\"train_loss\"],\n",
    "        \"val_loss\": checkpoint[\"val_loss\"],\n",
    "        \"train_miou\": checkpoint[\"train_miou\"],\n",
    "        \"val_miou\": checkpoint[\"val_miou\"],\n",
    "        \"train_acc\": checkpoint[\"train_acc\"],\n",
    "        \"val_acc\": checkpoint[\"val_acc\"],\n",
    "        \"lrs\": checkpoint[\"lrs\"],\n",
    "    }\n",
    "    return model, optimizer, scheduler, epoch, loss, history\n",
    "\n",
    "\n",
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    checkpoints = [\n",
    "        f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint_epoch_\")\n",
    "    ]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    latest_checkpoint = max(\n",
    "        checkpoints, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0])\n",
    "    )\n",
    "    return os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "\n",
    "\n",
    "def resume_from_checkpoint(model, optimizer, scheduler, checkpoint_dir):\n",
    "    latest_checkpoint = get_latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        model, optimizer, scheduler, start_epoch, min_loss, history = load_checkpoint(\n",
    "            model, optimizer, scheduler, latest_checkpoint\n",
    "        )\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        return model, optimizer, scheduler, start_epoch, min_loss, history\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "        return model, optimizer, scheduler, 0, float(\"inf\"), {}\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, loss, history, checkpoint_dir):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"loss\": loss,\n",
    "        \"train_loss\": history.get(\"train_loss\", []),\n",
    "        \"val_loss\": history.get(\"val_loss\", []),\n",
    "        \"train_miou\": history.get(\"train_miou\", []),\n",
    "        \"val_miou\": history.get(\"val_miou\", []),\n",
    "        \"train_acc\": history.get(\"train_acc\", []),\n",
    "        \"val_acc\": history.get(\"val_acc\", []),\n",
    "        \"lrs\": history.get(\"lrs\", []),\n",
    "    }\n",
    "    torch.save(\n",
    "        checkpoint, os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualization (`viz.py`)\n",
    "\n",
    "### Training Progress Visualization\n",
    "```python\n",
    "def plot_training_progress(history):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Loss curves\n",
    "    axes[0,0].plot(history['train_loss'], label='Training Loss')\n",
    "    axes[0,0].plot(history['val_loss'], label='Validation Loss')\n",
    "    axes[0,0].set_title('Loss vs Epochs')\n",
    "\n",
    "    # IoU curves\n",
    "    axes[0,1].plot(history['train_miou'], label='Training mIoU')\n",
    "    axes[0,1].plot(history['val_miou'], label='Validation mIoU')\n",
    "    axes[0,1].set_title('mIoU vs Epochs')\n",
    "\n",
    "    # Accuracy curves\n",
    "    axes[1,0].plot(history['train_acc'], label='Training Accuracy')\n",
    "    axes[1,0].plot(history['val_acc'], label='Validation Accuracy')\n",
    "    axes[1,0].set_title('Accuracy vs Epochs')\n",
    "\n",
    "    # Learning rate\n",
    "    axes[1,1].plot(history['lrs'], label='Learning Rate')\n",
    "    axes[1,1].set_title('Learning Rate vs Steps')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "### Prediction Visualization\n",
    "```python\n",
    "def visualize_prediction(image, mask, prediction):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image')\n",
    "\n",
    "    axes[1].imshow(mask)\n",
    "    axes[1].set_title('Ground Truth')\n",
    "\n",
    "    axes[2].imshow(prediction)\n",
    "    axes[2].set_title('Prediction')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "Features:\n",
    "- Training metrics visualization\n",
    "  - Loss curves\n",
    "  - IoU progress\n",
    "  - Accuracy tracking\n",
    "  - Learning rate schedule\n",
    "- Prediction visualization\n",
    "  - Side-by-side comparison\n",
    "  - Original image\n",
    "  - Ground truth mask\n",
    "  - Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"val_loss\"], label=\"val\", marker=\"o\")\n",
    "    plt.plot(history[\"train_loss\"], label=\"train\", marker=\"o\")\n",
    "    plt.title(\"Loss per epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_score(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"train_miou\"], label=\"train_mIoU\", marker=\"*\")\n",
    "    plt.plot(history[\"val_miou\"], label=\"val_mIoU\", marker=\"*\")\n",
    "    plt.title(\"Score per epoch\")\n",
    "    plt.ylabel(\"mean IoU\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_acc(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"train_acc\"], label=\"train_accuracy\", marker=\"*\")\n",
    "    plt.plot(history[\"val_acc\"], label=\"val_accuracy\", marker=\"*\")\n",
    "    plt.title(\"Accuracy per epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_predictions(\n",
    "    model,\n",
    "    test_set,\n",
    "    output_pdf,\n",
    "    num_classes=23,\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225],\n",
    "):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Create a colormap for the segmentation mask\n",
    "    cmap = plt.get_cmap(\"tab20\")\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, num_classes)]\n",
    "\n",
    "    with PdfPages(output_pdf) as pdf:\n",
    "        for i, (img, mask) in enumerate(test_set):\n",
    "            # Prepare the image\n",
    "            img_tensor = T.Compose([T.ToTensor(), T.Normalize(mean, std)])(img)\n",
    "            img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "            # Get the prediction\n",
    "            with torch.no_grad():\n",
    "                output = model(img_tensor)\n",
    "                pred_mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "            # Create a figure with three subplots\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            fig.suptitle(f\"Test Image {i+1}\")\n",
    "\n",
    "            # Plot original image\n",
    "            ax1.imshow(img)\n",
    "            ax1.set_title(\"Original Image\")\n",
    "            ax1.axis(\"off\")\n",
    "\n",
    "            # Plot ground truth mask\n",
    "            ax2.imshow(mask, cmap=cmap, vmin=0, vmax=num_classes - 1)\n",
    "            ax2.set_title(\"Ground Truth\")\n",
    "            ax2.axis(\"off\")\n",
    "\n",
    "            # Plot predicted mask\n",
    "            ax3.imshow(pred_mask, cmap=cmap, vmin=0, vmax=num_classes - 1)\n",
    "            ax3.set_title(\"Prediction\")\n",
    "            ax3.axis(\"off\")\n",
    "\n",
    "            # Add the plot to the PDF\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "\n",
    "        print(f\"Visualizations saved to {output_pdf}\")\n",
    "\n",
    "\n",
    "\n",
    "def visualize_sample(dataloader, num_classes=23):\n",
    "    \"\"\"Visualize a single sample image and its segmentation mask from a dataloader.\"\"\"\n",
    "    # Get a single batch from the dataloader\n",
    "    images, masks = next(iter(dataloader))\n",
    "\n",
    "    # Take the first image and mask from the batch\n",
    "    img = images[0].permute(1, 2, 0).numpy()  # Convert from CxHxW to HxWxC\n",
    "    mask = masks[0].numpy()\n",
    "\n",
    "    # Denormalize the image\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    # Create a colormap for the segmentation mask\n",
    "    cmap = plt.get_cmap(\"tab20\")\n",
    "\n",
    "    # Create a figure with three subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Plot original image\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title(\"Sample Image\")\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    # Plot segmentation mask\n",
    "    ax2.imshow(mask, cmap=cmap, vmin=0, vmax=num_classes - 1)\n",
    "    ax2.set_title(\"Segmentation Mask\")\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    # Plot overlay\n",
    "    ax3.imshow(img)\n",
    "    ax3.imshow(mask, cmap=cmap, vmin=0, vmax=num_classes - 1, alpha=0.5)\n",
    "    ax3.set_title(\"Overlay\")\n",
    "    ax3.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Dataset\n",
    "\n",
    "The next cell downloads a drone dataset from Kaggle and extracts it into a data directory.\n",
    "If the data directory already exists, it skips the download.\n",
    "The dataset is downloaded as a zip file and then extracted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data directory exists.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup path to data folder\n",
    "# data_path = Path(\"data/\")\n",
    "# image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "DATA_PATH = Path(\"data/\")\n",
    "\n",
    "# If the image folder doesn't exist, download it and prepare it...\n",
    "if Path(DATA_PATH).is_dir():\n",
    "    print(f\"{DATA_PATH} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {DATA_PATH} directory, creating one...\")\n",
    "    Path(DATA_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Download pizza, steak, sushi data\n",
    "    with open(DATA_PATH / \"archive.zip\", \"wb\") as f:\n",
    "        # request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "        request = requests.get(\"https://storage.googleapis.com/kaggle-data-sets/333968/1834160/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20241023%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20241023T113757Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=3e3bbcf7cb80c007d26471d7f7be115d075367ab5a6e241b83823607ac7683cb813a1757c5e71ee5052498b967758686f92be595272f684bf1a90bd8a21681ba7ba7a34e074464ac5e2d3b944af4ebf34d425d50281034b3fd3c17f1f15320f27eaf578cfbead4c6e40b721f1209333e55c6185b157001d9afd3762fd3f6eadb67ee4841ba059b999775c14615537f31e44b0f3e2cea010e3c13b612d18d952cf22c7d101962cdefe0da4d4e6a03345f9d3ceb14048de01e987345e318361b9d2f8cea7c9fb749de9c78eea4795da2e71ae5d8e065206627970bebb1eb523d7cf03d413978eb542f3f0500538b53bda10f198ca97f5f85d267bbe40b269487dd\")\n",
    "        print(\"Downloading drone dataset ...\")\n",
    "        f.write(request.content)\n",
    "\n",
    "    # Unzip pizza, steak, sushi data\n",
    "    with zipfile.ZipFile(DATA_PATH / \"archive.zip\", \"r\") as zip_ref:\n",
    "        print(\"Unzipping drone dataset ...\")\n",
    "        zip_ref.extractall(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from cnn_data import get_data_loaders\n",
    "# from model import create_model, fit, evaluate_model, resume_from_checkpoint\n",
    "# from viz import plot_loss, plot_score, plot_acc, visualize_predictions\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inference_only = True\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_set = get_data_loaders(batch_size=16)\n",
    "\n",
    "# Create model\n",
    "model = create_model()\n",
    "\n",
    "# Training parameters\n",
    "max_lr = 1e-3\n",
    "epochs = 4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not inference_only:\n",
    "    # Check for existing checkpoints and resume training if possible\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    model, optimizer, sched, start_epoch, min_loss, history = resume_from_checkpoint(\n",
    "        model, optimizer, sched, checkpoint_dir\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = fit(\n",
    "        epochs - start_epoch,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        sched,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "    )\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model, \"Unet-Mobilenet.pt\")\n",
    "\n",
    "    # Plot training results\n",
    "    plot_loss(history)\n",
    "    plot_score(history)\n",
    "    plot_acc(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Inference Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if inference_only:\n",
    "    # load model from .pt file\n",
    "    model = torch.load(\"Unet-Mobilenet.pt\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_miou, test_accuracy = evaluate_model(model, test_set)\n",
    "    print(\"Test Set mIoU:\", test_miou)\n",
    "    print(\"Test Set Pixel Accuracy:\", test_accuracy)\n",
    "\n",
    "    # Visualize predictions\n",
    "    visualize_predictions(model, test_set, \"test_predictions.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
