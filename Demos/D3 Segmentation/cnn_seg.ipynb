{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Image Segmentation Tutorial\n",
    "\n",
    "## Table of Contents\n",
    "- [CNN Image Segmentation Tutorial](#cnn-image-segmentation-tutorial)\n",
    "  - [Table of Contents](#table-of-contents)\n",
    "  - [Introduction](#introduction)\n",
    "    - [What is Image Segmentation?](#what-is-image-segmentation)\n",
    "    - [Types of Segmentation](#types-of-segmentation)\n",
    "    - [Real-world Applications](#real-world-applications)\n",
    "  - [Prerequisites](#prerequisites)\n",
    "    - [Required Libraries](#required-libraries)\n",
    "    - [Dataset Overview](#dataset-overview)\n",
    "  - [Data Preparation](#data-preparation)\n",
    "    - [Dataset Organization](#dataset-organization)\n",
    "    - [Custom Dataset Classes](#custom-dataset-classes)\n",
    "    - [Data Preprocessing](#data-preprocessing)\n",
    "    - [Data Augmentation](#data-augmentation)\n",
    "    - [DataLoader Creation](#dataloader-creation)\n",
    "  - [Model Architecture](#model-architecture)\n",
    "    - [U-Net with MobileNetV2](#u-net-with-mobilenetv2)\n",
    "      - [Architecture Components:](#architecture-components)\n",
    "    - [Training Components](#training-components)\n",
    "      - [Loss Function](#loss-function)\n",
    "      - [Optimizer](#optimizer)\n",
    "      - [Learning Rate Scheduler](#learning-rate-scheduler)\n",
    "    - [Model Configuration](#model-configuration)\n",
    "    - [Device Management](#device-management)\n",
    "  - [Metrics (`metrics.py`)](#metrics-metricspy)\n",
    "    - [Pixel Accuracy](#pixel-accuracy)\n",
    "    - [Mean IoU (Intersection over Union)](#mean-iou-intersection-over-union)\n",
    "  - [Training and Inference (`model.py`)](#training-and-inference-modelpy)\n",
    "    - [Training Pipeline](#training-pipeline)\n",
    "    - [Inference Pipeline](#inference-pipeline)\n",
    "  - [Visualization (`viz.py`)](#visualization-vizpy)\n",
    "    - [Training Progress Visualization](#training-progress-visualization)\n",
    "    - [Prediction Visualization](#prediction-visualization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Image segmentation is a fundamental computer vision task that involves dividing an\n",
    "image into multiple segments or regions, where each pixel in the image is assigned\n",
    "a class label. Unlike classification, which provides a single label for an entire\n",
    "image, or object detection, which identifies object locations with bounding boxes,\n",
    "segmentation provides pixel-level understanding of the image content.\n",
    "\n",
    "In this tutorial, we'll be working with semantic segmentation, where we assign\n",
    "each pixel to a predefined class category. For example, in our drone imagery\n",
    "dataset, pixels might belong to classes such as 'building', 'vegetation', 'road',\n",
    "or 'vehicle'.\n",
    "\n",
    "### Types of Segmentation\n",
    "There are several types of image segmentation, each serving different purposes:\n",
    "\n",
    "1. **Semantic Segmentation**\n",
    "   - Assigns each pixel to a class category\n",
    "   - Doesn't distinguish between instances of the same class\n",
    "   - Example: All 'car' pixels get the same label, regardless of how many cars\n",
    "     are present\n",
    "\n",
    "2. **Instance Segmentation**\n",
    "   - Identifies individual instances of objects\n",
    "   - Distinguishes between different instances of the same class\n",
    "   - Example: Each car in the image gets a unique instance ID\n",
    "\n",
    "3. **Panoptic Segmentation**\n",
    "   - Combines semantic segmentation for background classes\n",
    "   - Adds instance segmentation for countable objects\n",
    "   - Provides a unified view of scene understanding\n",
    "\n",
    "### Real-world Applications\n",
    "Image segmentation has numerous practical applications across various fields:\n",
    "\n",
    "1. **Aerial and Satellite Imagery**\n",
    "   - Urban planning and development\n",
    "   - Agricultural monitoring\n",
    "   - Disaster response and damage assessment\n",
    "   - Environmental monitoring\n",
    "\n",
    "2. **Medical Imaging**\n",
    "   - Tumor detection and measurement\n",
    "   - Organ segmentation\n",
    "   - Cell counting and analysis\n",
    "   - Surgical planning\n",
    "\n",
    "3. **Autonomous Vehicles**\n",
    "   - Road and lane detection\n",
    "   - Obstacle identification\n",
    "   - Pedestrian segmentation\n",
    "   - Traffic analysis\n",
    "\n",
    "4. **Industrial Applications**\n",
    "   - Quality control and inspection\n",
    "\n",
    "\n",
    "In this tutorial, we'll focus on semantic segmentation of aerial drone imagery,\n",
    "which has important applications in urban planning, mapping, and environmental\n",
    "monitoring. Our implementation uses a U-Net architecture with a MobileNetV2\n",
    "backbone, providing an efficient and effective solution for real-world\n",
    "segmentation tasks.\n",
    "\n",
    "\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "We'll be working with the Semantic Drone Dataset, which consists of:\n",
    "- Aerial imagery captured by drones\n",
    "- High-resolution RGB images (6000x4000px)\n",
    "- Pixel-wise semantic segmentation masks\n",
    "- 23 different class categories including:\n",
    "  - Buildings\n",
    "  - Roads\n",
    "  - Vegetation\n",
    "  - Vehicles\n",
    "  - People\n",
    "  - Other urban features\n",
    "\n",
    "The dataset structure:\n",
    "```\n",
    "data/\n",
    "├── dataset/\n",
    "│   └── semantic_drone_dataset/\n",
    "│       ├── original_images/     # RGB images\n",
    "│       └── label_images_semantic/  # Segmentation masks\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before diving into the implementation, let's review the essential libraries and tools we'll be using in this project.\n",
    "\n",
    "### Required Libraries\n",
    "\n",
    "1. **PyTorch**\n",
    "   - Deep learning framework\n",
    "   - Provides neural network building blocks and GPU acceleration\n",
    "   - Version requirement: >= 1.7.0\n",
    "\n",
    "2. **OpenCV (cv2)**\n",
    "   - Image loading and preprocessing\n",
    "   - Color space conversions\n",
    "   - Basic image operations\n",
    "   ```python\n",
    "   pip install opencv-python\n",
    "   ```\n",
    "\n",
    "3. **Albumentations**\n",
    "   - High-performance data augmentation\n",
    "   - Implements various image transformations\n",
    "   - Specially designed for segmentation tasks\n",
    "   ```python\n",
    "   pip install albumentations\n",
    "   ```\n",
    "\n",
    "4. **Segmentation Models PyTorch (smp)**\n",
    "   - Pre-implemented segmentation architectures\n",
    "   - Provides pre-trained encoders\n",
    "   - Easy-to-use interface for segmentation models\n",
    "   ```python\n",
    "   pip install segmentation-models-pytorch\n",
    "   ```\n",
    "\n",
    "5. **Supporting Libraries**\n",
    "   - NumPy: Numerical operations and array manipulation\n",
    "   - Pandas: Data organization and splitting\n",
    "   - Matplotlib: Visualization and result plotting\n",
    "   - tqdm: Progress bar for training loops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preparation\n",
    "\n",
    "The data preparation pipeline is implemented in `cnn_data.py` and handles dataset organization, loading, and preprocessing.\n",
    "\n",
    "### Dataset Organization\n",
    "\n",
    "First, we define the paths to our image and mask directories:\n",
    "\n",
    "```python\n",
    "IMAGE_PATH = \"data/dataset/semantic_drone_dataset/original_images/\"\n",
    "MASK_PATH = \"data/dataset/semantic_drone_dataset/label_images_semantic/\"\n",
    "```\n",
    "\n",
    "We create a DataFrame to organize our data and split it into training, validation, and test sets:\n",
    "\n",
    "```python\n",
    "def create_df():\n",
    "    name = []\n",
    "    for dirname, _, filenames in os.walk(IMAGE_PATH):\n",
    "        for filename in filenames:\n",
    "            name.append(filename.split(\".\")[0])\n",
    "    return pd.DataFrame({\"id\": name}, index=np.arange(0, len(name)))\n",
    "\n",
    "def get_data_splits():\n",
    "    df = create_df()\n",
    "    # Split: 75% train, 15% validation, 10% test\n",
    "    X_trainval, X_test = train_test_split(df[\"id\"].values, test_size=0.1, random_state=19)\n",
    "    X_train, X_val = train_test_split(X_trainval, test_size=0.15, random_state=19)\n",
    "    return X_train, X_val, X_test\n",
    "```\n",
    "\n",
    "### Custom Dataset Classes\n",
    "\n",
    "We implement two custom PyTorch Dataset classes:\n",
    "\n",
    "1. **DroneDataset**: For training and validation data\n",
    "```python\n",
    "class DroneDataset(Dataset):\n",
    "    def __init__(self, img_path, mask_path, X, mean, std, transform=None, patch=False):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "        self.patches = patch\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        img = cv2.imread(self.img_path + self.X[idx] + \".jpg\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_path + self.X[idx] + \".png\",\n",
    "                         cv2.IMREAD_GRAYSCALE)\n",
    "```\n",
    "\n",
    "2. **DroneTestDataset**: Specialized for test data with simplified processing\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "Our preprocessing pipeline includes:\n",
    "\n",
    "1. **Image Loading**\n",
    "   - Read images using OpenCV\n",
    "   - Convert from BGR to RGB color space\n",
    "   - Load masks as grayscale images\n",
    "\n",
    "2. **Normalization**\n",
    "   - Standardize images using ImageNet statistics:\n",
    "   ```python\n",
    "   mean = [0.485, 0.456, 0.406]\n",
    "   std = [0.229, 0.224, 0.225]\n",
    "   ```\n",
    "\n",
    "3. **Tensor Conversion**\n",
    "   - Convert images to PyTorch tensors\n",
    "   - Convert masks to long tensors for classification\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "We use the Albumentations library for efficient data augmentation:\n",
    "\n",
    "```python\n",
    "t_train = A.Compose([\n",
    "    A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST),\n",
    "    A.HorizontalFlip(),\n",
    "    A.VerticalFlip(),\n",
    "    A.GridDistortion(p=0.2),\n",
    "    A.RandomBrightnessContrast((0, 0.5), (0, 0.5)),\n",
    "    A.GaussNoise(),\n",
    "])\n",
    "\n",
    "t_val = A.Compose([\n",
    "    A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST),\n",
    "    A.HorizontalFlip(),\n",
    "    A.GridDistortion(p=0.2),\n",
    "])\n",
    "```\n",
    "\n",
    "The augmentation pipeline includes:\n",
    "- Resizing to a consistent size\n",
    "- Random horizontal and vertical flips\n",
    "- Grid distortion for geometric variety\n",
    "- Brightness and contrast adjustments\n",
    "- Gaussian noise for robustness\n",
    "\n",
    "### DataLoader Creation\n",
    "\n",
    "Finally, we create PyTorch DataLoaders for efficient batch processing:\n",
    "\n",
    "```python\n",
    "def get_data_loaders(batch_size=16):\n",
    "    X_train, X_val, X_test = get_data_splits()\n",
    "\n",
    "    train_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_train,\n",
    "                            mean, std, t_train, patch=False)\n",
    "    val_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_val,\n",
    "                          mean, std, t_val, patch=False)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, val_loader, test_set\n",
    "```\n",
    "\n",
    "This setup provides:\n",
    "- Batched data loading\n",
    "- Automatic shuffling\n",
    "- Parallel data loading capabilities\n",
    "- Memory-efficient data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "\n",
    "IMAGE_PATH = \"data/dataset/semantic_drone_dataset/original_images/\"\n",
    "MASK_PATH = \"data/dataset/semantic_drone_dataset/label_images_semantic/\"\n",
    "\n",
    "\n",
    "def create_df():\n",
    "    name = []\n",
    "    for dirname, _, filenames in os.walk(IMAGE_PATH):\n",
    "        for filename in filenames:\n",
    "            name.append(filename.split(\".\")[0])\n",
    "    return pd.DataFrame({\"id\": name}, index=np.arange(0, len(name)))\n",
    "\n",
    "\n",
    "def get_data_splits():\n",
    "    df = create_df()\n",
    "    X_trainval, X_test = train_test_split(\n",
    "        df[\"id\"].values, test_size=0.1, random_state=19\n",
    "    )\n",
    "    X_train, X_val = train_test_split(X_trainval, test_size=0.15, random_state=19)\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "\n",
    "class DroneDataset(Dataset):\n",
    "    def __init__(self, img_path, mask_path, X, mean, std, transform=None, patch=False):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "        self.patches = patch\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_path + self.X[idx] + \".jpg\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_path + self.X[idx] + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img, mask=mask)\n",
    "            img = Image.fromarray(aug[\"image\"])\n",
    "            mask = aug[\"mask\"]\n",
    "\n",
    "        if self.transform is None:\n",
    "            img = Image.fromarray(img)\n",
    "\n",
    "        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n",
    "        img = t(img)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        if self.patches:\n",
    "            img, mask = self.tiles(img, mask)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "    def tiles(self, img, mask):\n",
    "        img_patches = img.unfold(1, 512, 512).unfold(2, 768, 768)\n",
    "        img_patches = img_patches.contiguous().view(3, -1, 512, 768)\n",
    "        img_patches = img_patches.permute(1, 0, 2, 3)\n",
    "\n",
    "        mask_patches = mask.unfold(0, 512, 512).unfold(1, 768, 768)\n",
    "        mask_patches = mask_patches.contiguous().view(-1, 512, 768)\n",
    "\n",
    "        return img_patches, mask_patches\n",
    "\n",
    "\n",
    "class DroneTestDataset(Dataset):\n",
    "    def __init__(self, img_path, mask_path, X, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_path + self.X[idx] + \".jpg\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_path + self.X[idx] + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img, mask=mask)\n",
    "            img = Image.fromarray(aug[\"image\"])\n",
    "            mask = aug[\"mask\"]\n",
    "\n",
    "        if self.transform is None:\n",
    "            img = Image.fromarray(img)\n",
    "\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "def get_data_loaders(batch_size=16):\n",
    "    X_train, X_val, X_test = get_data_splits()\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    t_train = A.Compose(\n",
    "        [\n",
    "            A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST),\n",
    "            A.HorizontalFlip(),\n",
    "            A.VerticalFlip(),\n",
    "            A.GridDistortion(p=0.2),\n",
    "            A.RandomBrightnessContrast((0, 0.5), (0, 0.5)),\n",
    "            A.GaussNoise(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    t_val = A.Compose(\n",
    "        [\n",
    "            A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST),\n",
    "            A.HorizontalFlip(),\n",
    "            A.GridDistortion(p=0.2),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_set = DroneDataset(\n",
    "        IMAGE_PATH, MASK_PATH, X_train, mean, std, t_train, patch=False\n",
    "    )\n",
    "    val_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_val, mean, std, t_val, patch=False)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    t_test = A.Resize(768, 1152, interpolation=cv2.INTER_NEAREST)\n",
    "    test_set = DroneTestDataset(IMAGE_PATH, MASK_PATH, X_test, transform=t_test)\n",
    "\n",
    "    return train_loader, val_loader, test_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics (`metrics.py`)\n",
    "\n",
    "Our implementation uses two key metrics to evaluate segmentation performance:\n",
    "\n",
    "### Pixel Accuracy\n",
    "```python\n",
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy\n",
    "```\n",
    "\n",
    "- Measures percentage of correctly classified pixels\n",
    "- Simple but potentially misleading for imbalanced classes\n",
    "- Range: [0, 1], where 1 is perfect classification\n",
    "\n",
    "### Mean IoU (Intersection over Union)\n",
    "```python\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes):\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0:\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union + smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)\n",
    "```\n",
    "\n",
    "- Calculates intersection over union for each class\n",
    "- Better metric for imbalanced datasets\n",
    "- Handles missing classes with NaN values\n",
    "- Range: [0, 1], where 1 is perfect segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes):  # loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0:  # no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = (\n",
    "                    torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                )\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union + smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "Our implementation uses a U-Net architecture with a MobileNetV2 backbone, leveraging the segmentation-models-pytorch (smp) library for efficient implementation.\n",
    "\n",
    "### U-Net with MobileNetV2\n",
    "\n",
    "The model is created using the following configuration:\n",
    "\n",
    "```python\n",
    "def create_model():\n",
    "    return smp.Unet(\n",
    "        \"mobilenet_v2\",          # Encoder backbone\n",
    "        encoder_weights=\"imagenet\",  # Pre-trained weights\n",
    "        classes=23,              # Number of output classes\n",
    "        activation=None,         # No activation (handled by loss function)\n",
    "        encoder_depth=5,         # Number of encoder stages\n",
    "        decoder_channels=[256, 128, 64, 32, 16]  # Decoder channel sizes\n",
    "    )\n",
    "```\n",
    "\n",
    "#### Architecture Components:\n",
    "\n",
    "1. **Encoder (MobileNetV2)**\n",
    "   - Pre-trained on ImageNet\n",
    "   - Efficient mobile-first architecture\n",
    "   - Feature extraction at multiple scales\n",
    "   - 5 stages of downsampling\n",
    "\n",
    "2. **Decoder**\n",
    "   - Progressive upsampling path\n",
    "   - Channel sizes: [256, 128, 64, 32, 16]\n",
    "   - Skip connections from encoder\n",
    "   - Feature refinement at each stage\n",
    "\n",
    "3. **Output Layer**\n",
    "   - 23 output channels (one per class)\n",
    "   - No activation (handled by CrossEntropyLoss)\n",
    "\n",
    "### Training Components\n",
    "\n",
    "#### Loss Function\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "```\n",
    "- Combines LogSoftmax and NLLLoss\n",
    "- Handles multi-class segmentation\n",
    "- Automatically normalizes class predictions\n",
    "\n",
    "#### Optimizer\n",
    "```python\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=max_lr,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "```\n",
    "- AdamW optimizer for better generalization\n",
    "- Weight decay for regularization\n",
    "- Adaptive learning rate adjustments\n",
    "\n",
    "#### Learning Rate Scheduler\n",
    "```python\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=len(train_loader)\n",
    ")\n",
    "```\n",
    "- One Cycle learning rate policy\n",
    "- Gradual warmup and cooldown\n",
    "- Helps prevent overfitting\n",
    "- Improves training stability\n",
    "\n",
    "### Model Configuration\n",
    "\n",
    "Key hyperparameters for the model:\n",
    "```python\n",
    "max_lr = 1e-3        # Maximum learning rate\n",
    "epochs = 30          # Number of training epochs\n",
    "weight_decay = 1e-4  # L2 regularization factor\n",
    "batch_size = 16      # Images per batch\n",
    "```\n",
    "\n",
    "### Device Management\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move model to GPU if available\n",
    "```\n",
    "\n",
    "This architecture provides:\n",
    "- Efficient feature extraction through MobileNetV2\n",
    "- High-resolution detail through skip connections\n",
    "- Memory-efficient training\n",
    "- Fast inference capabilities\n",
    "- Good balance of accuracy and computational cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training and Inference (`model.py`)\n",
    "\n",
    "### Training Pipeline\n",
    "```python\n",
    "def train_model(train_loader, val_loader, epochs=30, checkpoint_dir=\"checkpoints\"):\n",
    "    model = create_model()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n",
    "                                                  steps_per_epoch=len(train_loader))\n",
    "\n",
    "    history = fit(epochs, model, train_loader, val_loader, criterion,\n",
    "                 optimizer, scheduler, checkpoint_dir=checkpoint_dir)\n",
    "    return model, history\n",
    "```\n",
    "\n",
    "Key components:\n",
    "- Model creation with MobileNetV2 backbone\n",
    "- CrossEntropyLoss for multi-class segmentation\n",
    "- AdamW optimizer with OneCycleLR scheduler\n",
    "- Checkpoint management for model saving/loading\n",
    "\n",
    "### Inference Pipeline\n",
    "```python\n",
    "def inference(model_path, image_path):\n",
    "    model = create_model()\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image = load_image(image_path)\n",
    "        prediction = model(image.unsqueeze(0))\n",
    "        prediction = torch.argmax(prediction.squeeze(), dim=0)\n",
    "    return prediction\n",
    "```\n",
    "\n",
    "Features:\n",
    "- Model loading from checkpoint\n",
    "- Single image prediction\n",
    "- No gradient computation during inference\n",
    "- Returns class predictions per pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from metrics import mIoU, pixel_accuracy\n",
    "import os\n",
    "from torchvision import transforms  # Change this line\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    return smp.Unet(\n",
    "        \"mobilenet_v2\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        classes=23,\n",
    "        activation=None,\n",
    "        encoder_depth=5,\n",
    "        decoder_channels=[256, 128, 64, 32, 16],\n",
    "    )\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "\n",
    "def fit(\n",
    "    epochs,\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    patch=False,\n",
    "    checkpoint_dir=\"checkpoints\",\n",
    "):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_iou = []\n",
    "    val_acc = []\n",
    "    train_iou = []\n",
    "    train_acc = []\n",
    "    lrs = []\n",
    "    min_loss = np.inf\n",
    "    decrease = 1\n",
    "    not_improve = 0\n",
    "\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    model.to(device)\n",
    "    fit_time = time.time()\n",
    "    for e in range(epochs):\n",
    "        since = time.time()\n",
    "        running_loss = 0\n",
    "        iou_score = 0\n",
    "        accuracy = 0\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for i, data in enumerate(tqdm(train_loader)):\n",
    "            # training phase\n",
    "            image_tiles, mask_tiles = data\n",
    "            if patch:\n",
    "                bs, n_tiles, c, h, w = image_tiles.size()\n",
    "                image_tiles = image_tiles.view(-1, c, h, w)\n",
    "                mask_tiles = mask_tiles.view(-1, h, w)\n",
    "\n",
    "            image = image_tiles.to(device)\n",
    "            mask = mask_tiles.to(device)\n",
    "            # forward\n",
    "            output = model(image)\n",
    "            loss = criterion(output, mask)\n",
    "            # evaluation metrics\n",
    "            iou_score += mIoU(output, mask)\n",
    "            accuracy += pixel_accuracy(output, mask)\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()  # update weight\n",
    "            optimizer.zero_grad()  # reset gradient\n",
    "\n",
    "            # step the learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        else:\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            test_accuracy = 0\n",
    "            val_iou_score = 0\n",
    "            # validation loop\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(tqdm(val_loader)):\n",
    "                    # reshape to 9 patches from single image, delete batch size\n",
    "                    image_tiles, mask_tiles = data\n",
    "\n",
    "                    if patch:\n",
    "                        bs, n_tiles, c, h, w = image_tiles.size()\n",
    "                        image_tiles = image_tiles.view(-1, c, h, w)\n",
    "                        mask_tiles = mask_tiles.view(-1, h, w)\n",
    "\n",
    "                    image = image_tiles.to(device)\n",
    "                    mask = mask_tiles.to(device)\n",
    "                    output = model(image)\n",
    "                    # evaluation metrics\n",
    "                    val_iou_score += mIoU(output, mask)\n",
    "                    test_accuracy += pixel_accuracy(output, mask)\n",
    "                    # loss\n",
    "                    loss = criterion(output, mask)\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "            # calculation mean for each batch\n",
    "            train_losses.append(running_loss / len(train_loader))\n",
    "            test_losses.append(test_loss / len(val_loader))\n",
    "\n",
    "            if min_loss > (test_loss / len(val_loader)):\n",
    "                print(\n",
    "                    \"Loss Decreasing.. {:.3f} >> {:.3f} \".format(\n",
    "                        min_loss, (test_loss / len(val_loader))\n",
    "                    )\n",
    "                )\n",
    "                min_loss = test_loss / len(val_loader)\n",
    "                decrease += 1\n",
    "                if decrease % 5 == 0:\n",
    "                    print(\"saving model...\")\n",
    "                    torch.save(\n",
    "                        model,\n",
    "                        \"Unet-Mobilenet_v2_mIoU-{:.3f}.pt\".format(\n",
    "                            val_iou_score / len(val_loader)\n",
    "                        ),\n",
    "                    )\n",
    "\n",
    "            if (test_loss / len(val_loader)) > min_loss:\n",
    "                not_improve += 1\n",
    "                min_loss = test_loss / len(val_loader)\n",
    "                print(f\"Loss Not Decrease for {not_improve} time\")\n",
    "                if not_improve == 7:\n",
    "                    print(\"Loss not decrease for 7 times, Stop Training\")\n",
    "                    break\n",
    "\n",
    "            # iou\n",
    "            val_iou.append(val_iou_score / len(val_loader))\n",
    "            train_iou.append(iou_score / len(train_loader))\n",
    "            train_acc.append(accuracy / len(train_loader))\n",
    "            val_acc.append(test_accuracy / len(val_loader))\n",
    "            print(\n",
    "                \"Epoch:{}/{}..\".format(e + 1, epochs),\n",
    "                \"Train Loss: {:.3f}..\".format(running_loss / len(train_loader)),\n",
    "                \"Val Loss: {:.3f}..\".format(test_loss / len(val_loader)),\n",
    "                \"Train mIoU:{:.3f}..\".format(iou_score / len(train_loader)),\n",
    "                \"Val mIoU: {:.3f}..\".format(val_iou_score / len(val_loader)),\n",
    "                \"Train Acc:{:.3f}..\".format(accuracy / len(train_loader)),\n",
    "                \"Val Acc:{:.3f}..\".format(test_accuracy / len(val_loader)),\n",
    "                \"Time: {:.2f}m\".format((time.time() - since) / 60),\n",
    "            )\n",
    "\n",
    "        # Save checkpoint\n",
    "        history = {\n",
    "            \"train_loss\": train_losses,\n",
    "            \"val_loss\": test_losses,\n",
    "            \"train_miou\": train_iou,\n",
    "            \"val_miou\": val_iou,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"lrs\": lrs,\n",
    "        }\n",
    "        save_checkpoint(\n",
    "            model, optimizer, scheduler, e + 1, min_loss, history, checkpoint_dir\n",
    "        )\n",
    "\n",
    "    print(\"Total time: {:.2f} m\".format((time.time() - fit_time) / 60))\n",
    "    return history\n",
    "\n",
    "\n",
    "def predict_image_mask_miou(\n",
    "    model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "):\n",
    "    model.eval()\n",
    "    t = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize(mean, std)]\n",
    "    )  # Change this line\n",
    "    image = t(image)\n",
    "    model.to(device)\n",
    "    image = image.to(device)\n",
    "    mask = mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0)\n",
    "        mask = mask.unsqueeze(0)\n",
    "        output = model(image)\n",
    "        score = mIoU(output, mask)\n",
    "        masked = torch.argmax(output, dim=1)\n",
    "        masked = masked.cpu().squeeze(0)\n",
    "    return masked, score\n",
    "\n",
    "\n",
    "def predict_image_mask_pixel(\n",
    "    model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "):\n",
    "    model.eval()\n",
    "    t = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize(mean, std)]\n",
    "    )  # Change this line\n",
    "    image = t(image)\n",
    "    model.to(device)\n",
    "    image = image.to(device)\n",
    "    mask = mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0)\n",
    "        mask = mask.unsqueeze(0)\n",
    "        output = model(image)\n",
    "        acc = pixel_accuracy(output, mask)\n",
    "        masked = torch.argmax(output, dim=1)\n",
    "        masked = masked.cpu().squeeze(0)\n",
    "    return masked, acc\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_set):\n",
    "    score_iou = []\n",
    "    accuracy = []\n",
    "    for i in tqdm(range(len(test_set))):\n",
    "        img, mask = test_set[i]\n",
    "        pred_mask, score = predict_image_mask_miou(model, img, mask)\n",
    "        score_iou.append(score)\n",
    "        _, acc = predict_image_mask_pixel(model, img, mask)\n",
    "        accuracy.append(acc)\n",
    "    return np.mean(score_iou), np.mean(accuracy)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    loss = checkpoint[\"loss\"]\n",
    "    history = {\n",
    "        \"train_loss\": checkpoint[\"train_loss\"],\n",
    "        \"val_loss\": checkpoint[\"val_loss\"],\n",
    "        \"train_miou\": checkpoint[\"train_miou\"],\n",
    "        \"val_miou\": checkpoint[\"val_miou\"],\n",
    "        \"train_acc\": checkpoint[\"train_acc\"],\n",
    "        \"val_acc\": checkpoint[\"val_acc\"],\n",
    "        \"lrs\": checkpoint[\"lrs\"],\n",
    "    }\n",
    "    return model, optimizer, scheduler, epoch, loss, history\n",
    "\n",
    "\n",
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    checkpoints = [\n",
    "        f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint_epoch_\")\n",
    "    ]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    latest_checkpoint = max(\n",
    "        checkpoints, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0])\n",
    "    )\n",
    "    return os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "\n",
    "\n",
    "def resume_from_checkpoint(model, optimizer, scheduler, checkpoint_dir):\n",
    "    latest_checkpoint = get_latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        model, optimizer, scheduler, start_epoch, min_loss, history = load_checkpoint(\n",
    "            model, optimizer, scheduler, latest_checkpoint\n",
    "        )\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        return model, optimizer, scheduler, start_epoch, min_loss, history\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "        return model, optimizer, scheduler, 0, float(\"inf\"), {}\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, loss, history, checkpoint_dir):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"loss\": loss,\n",
    "        \"train_loss\": history.get(\"train_loss\", []),\n",
    "        \"val_loss\": history.get(\"val_loss\", []),\n",
    "        \"train_miou\": history.get(\"train_miou\", []),\n",
    "        \"val_miou\": history.get(\"val_miou\", []),\n",
    "        \"train_acc\": history.get(\"train_acc\", []),\n",
    "        \"val_acc\": history.get(\"val_acc\", []),\n",
    "        \"lrs\": history.get(\"lrs\", []),\n",
    "    }\n",
    "    torch.save(\n",
    "        checkpoint, os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualization (`viz.py`)\n",
    "\n",
    "### Training Progress Visualization\n",
    "```python\n",
    "def plot_training_progress(history):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Loss curves\n",
    "    axes[0,0].plot(history['train_loss'], label='Training Loss')\n",
    "    axes[0,0].plot(history['val_loss'], label='Validation Loss')\n",
    "    axes[0,0].set_title('Loss vs Epochs')\n",
    "\n",
    "    # IoU curves\n",
    "    axes[0,1].plot(history['train_miou'], label='Training mIoU')\n",
    "    axes[0,1].plot(history['val_miou'], label='Validation mIoU')\n",
    "    axes[0,1].set_title('mIoU vs Epochs')\n",
    "\n",
    "    # Accuracy curves\n",
    "    axes[1,0].plot(history['train_acc'], label='Training Accuracy')\n",
    "    axes[1,0].plot(history['val_acc'], label='Validation Accuracy')\n",
    "    axes[1,0].set_title('Accuracy vs Epochs')\n",
    "\n",
    "    # Learning rate\n",
    "    axes[1,1].plot(history['lrs'], label='Learning Rate')\n",
    "    axes[1,1].set_title('Learning Rate vs Steps')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "### Prediction Visualization\n",
    "```python\n",
    "def visualize_prediction(image, mask, prediction):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image')\n",
    "\n",
    "    axes[1].imshow(mask)\n",
    "    axes[1].set_title('Ground Truth')\n",
    "\n",
    "    axes[2].imshow(prediction)\n",
    "    axes[2].set_title('Prediction')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "Features:\n",
    "- Training metrics visualization\n",
    "  - Loss curves\n",
    "  - IoU progress\n",
    "  - Accuracy tracking\n",
    "  - Learning rate schedule\n",
    "- Prediction visualization\n",
    "  - Side-by-side comparison\n",
    "  - Original image\n",
    "  - Ground truth mask\n",
    "  - Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"val_loss\"], label=\"val\", marker=\"o\")\n",
    "    plt.plot(history[\"train_loss\"], label=\"train\", marker=\"o\")\n",
    "    plt.title(\"Loss per epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_score(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"train_miou\"], label=\"train_mIoU\", marker=\"*\")\n",
    "    plt.plot(history[\"val_miou\"], label=\"val_mIoU\", marker=\"*\")\n",
    "    plt.title(\"Score per epoch\")\n",
    "    plt.ylabel(\"mean IoU\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_acc(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history[\"train_acc\"], label=\"train_accuracy\", marker=\"*\")\n",
    "    plt.plot(history[\"val_acc\"], label=\"val_accuracy\", marker=\"*\")\n",
    "    plt.title(\"Accuracy per epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_predictions(\n",
    "    model,\n",
    "    test_set,\n",
    "    output_pdf,\n",
    "    num_classes=23,\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225],\n",
    "):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Create a colormap for the segmentation mask\n",
    "    cmap = plt.get_cmap(\"tab20\")\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, num_classes)]\n",
    "\n",
    "    with PdfPages(output_pdf) as pdf:\n",
    "        for i, (img, mask) in enumerate(test_set):\n",
    "            # Prepare the image\n",
    "            img_tensor = T.Compose([T.ToTensor(), T.Normalize(mean, std)])(img)\n",
    "            img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "            # Get the prediction\n",
    "            with torch.no_grad():\n",
    "                output = model(img_tensor)\n",
    "                pred_mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "            # Create a figure with three subplots\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            fig.suptitle(f\"Test Image {i+1}\")\n",
    "\n",
    "            # Plot original image\n",
    "            ax1.imshow(img)\n",
    "            ax1.set_title(\"Original Image\")\n",
    "            ax1.axis(\"off\")\n",
    "\n",
    "            # Plot ground truth mask\n",
    "            ax2.imshow(mask, cmap=cmap, vmin=0, vmax=num_classes - 1)\n",
    "            ax2.set_title(\"Ground Truth\")\n",
    "            ax2.axis(\"off\")\n",
    "\n",
    "            # Plot predicted mask\n",
    "            ax3.imshow(pred_mask, cmap=cmap, vmin=0, vmax=num_classes - 1)\n",
    "            ax3.set_title(\"Prediction\")\n",
    "            ax3.axis(\"off\")\n",
    "\n",
    "            # Add the plot to the PDF\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "\n",
    "        print(f\"Visualizations saved to {output_pdf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Dataset\n",
    "\n",
    "The next cell downloads a drone dataset from Kaggle and extracts it into a data directory.\n",
    "If the data directory already exists, it skips the download.\n",
    "The dataset is downloaded as a zip file and then extracted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data directory exists.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup path to data folder\n",
    "# data_path = Path(\"data/\")\n",
    "# image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "DATA_PATH = Path(\"data/\")\n",
    "\n",
    "# If the image folder doesn't exist, download it and prepare it...\n",
    "if Path(DATA_PATH).is_dir():\n",
    "    print(f\"{DATA_PATH} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {DATA_PATH} directory, creating one...\")\n",
    "    Path(DATA_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Download pizza, steak, sushi data\n",
    "    with open(DATA_PATH / \"archive.zip\", \"wb\") as f:\n",
    "        # request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "        request = requests.get(\"https://storage.googleapis.com/kaggle-data-sets/333968/1834160/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20241023%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20241023T113757Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=3e3bbcf7cb80c007d26471d7f7be115d075367ab5a6e241b83823607ac7683cb813a1757c5e71ee5052498b967758686f92be595272f684bf1a90bd8a21681ba7ba7a34e074464ac5e2d3b944af4ebf34d425d50281034b3fd3c17f1f15320f27eaf578cfbead4c6e40b721f1209333e55c6185b157001d9afd3762fd3f6eadb67ee4841ba059b999775c14615537f31e44b0f3e2cea010e3c13b612d18d952cf22c7d101962cdefe0da4d4e6a03345f9d3ceb14048de01e987345e318361b9d2f8cea7c9fb749de9c78eea4795da2e71ae5d8e065206627970bebb1eb523d7cf03d413978eb542f3f0500538b53bda10f198ca97f5f85d267bbe40b269487dd\")\n",
    "        print(\"Downloading drone dataset ...\")\n",
    "        f.write(request.content)\n",
    "\n",
    "    # Unzip pizza, steak, sushi data\n",
    "    with zipfile.ZipFile(DATA_PATH / \"archive.zip\", \"r\") as zip_ref:\n",
    "        print(\"Unzipping drone dataset ...\")\n",
    "        zip_ref.extractall(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Starting from scratch.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from cnn_data import get_data_loaders\n",
    "# from model import create_model, fit, evaluate_model, resume_from_checkpoint\n",
    "# from viz import plot_loss, plot_score, plot_acc, visualize_predictions\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_set = get_data_loaders(batch_size=16)\n",
    "\n",
    "# Create model\n",
    "model = create_model()\n",
    "\n",
    "# Training parameters\n",
    "max_lr = 1e-3\n",
    "epochs = 4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader)\n",
    ")\n",
    "\n",
    "# Check for existing checkpoints and resume training if possible\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "model, optimizer, sched, start_epoch, min_loss, history = resume_from_checkpoint(\n",
    "    model, optimizer, sched, checkpoint_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 20/20 [02:02<00:00,  6.14s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 4/4 [00:18<00:00,  4.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Decreasing.. inf >> 5.935 \n",
      "Epoch:1/4.. Train Loss: 2.741.. Val Loss: 5.935.. Train mIoU:0.048.. Val mIoU: 0.076.. Train Acc:0.272.. Val Acc:0.496.. Time: 2.36m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 1/20 [00:11<03:41, 11.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnet-Mobilenet.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[52], line 68\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, patch, checkpoint_dir)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[1;32m     67\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 68\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# training phase\u001b[39;49;00m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_tiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_tiles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpatch\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml311/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml311/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/ml311/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[50], line 47\u001b[0m, in \u001b[0;36mDroneDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 47\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     49\u001b[0m     mask \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX[idx] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "history = fit(\n",
    "    epochs - start_epoch,\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    sched,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model, \"Unet-Mobilenet.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "plot_loss(history)\n",
    "plot_score(history)\n",
    "plot_acc(history)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_miou, test_accuracy = evaluate_model(model, test_set)\n",
    "print(\"Test Set mIoU:\", test_miou)\n",
    "print(\"Test Set Pixel Accuracy:\", test_accuracy)\n",
    "\n",
    "# Visualize predictions\n",
    "visualize_predictions(model, test_set, \"test_predictions.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
